{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# python modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from os import rename, times\r\n",
    "# from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import math\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# loding data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# creating model\r\n",
    "df = pd.read_csv('merged-1-to-16.csv')\r\n",
    "#remove data of unrated round like technocup etc.....\r\n",
    "# df=df.reset_index()\r\n",
    "# print(df.head())\r\n",
    "# droping all row having any negative value in column\r\n",
    "df = df[df.select_dtypes(include=[np.number]).ge(0).all(1)]\r\n",
    "# droping row having avg solving time is gratar than 10 hr and less than equal to 0\r\n",
    "df = df.loc[(df['avg_solving_time'] <= 18000) & (df['avg_solving_time'] > 0)]\r\n",
    "df_div1 = df.loc[(df['contest_type'] == 1)]\r\n",
    "df_div2 = df.loc[(df['contest_type'] == 2)]\r\n",
    "df_div3 = df.loc[(df['contest_type'] == 3)]\r\n",
    "# df_div3.describe()\r\n",
    "# df.head()\r\n",
    "# df.describe()\r\n",
    "# print(df.head())\r\n",
    "print(df.describe())\r\n",
    "q_1 = df.loc[:, ['question_no', 'q_rating', 'contest_type']]\r\n",
    "q_1 = q_1.loc[(q_1['question_no'] == 1) & (q_1['contest_type'] == 1)]\r\n",
    "# print(q_1)\r\n",
    "# print(q_1.describe())\r\n",
    "x = df.drop(['q_rating'], axis=1)\r\n",
    "x = x.drop(['contest_id'], axis=1)\r\n",
    "y = df['q_rating']\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        contest_id     q_rating  contest_type  question_no     accuracy  \\\n",
      "count  4700.000000  4700.000000   4700.000000  4700.000000  4700.000000   \n",
      "mean   1102.369149  1891.595745      2.234894     3.822340     0.634800   \n",
      "std     251.454666   677.331562      1.077807     2.253873     0.258493   \n",
      "min     641.000000   800.000000      0.000000     1.000000     0.002401   \n",
      "25%     886.000000  1400.000000      2.000000     2.000000     0.457588   \n",
      "50%    1103.500000  1900.000000      2.000000     4.000000     0.678199   \n",
      "75%    1322.000000  2400.000000      3.000000     5.000000     0.853926   \n",
      "max    1529.000000  3500.000000      4.000000    16.000000     1.000000   \n",
      "\n",
      "       avg_solving_time    sub_ratio  tried_but_cannot_solved  \\\n",
      "count       4700.000000  4700.000000              4700.000000   \n",
      "mean        2451.258099     0.327319               446.673830   \n",
      "std         1302.181881     0.330178               694.320172   \n",
      "min            9.000000     0.000070                 0.000000   \n",
      "25%         1661.950253     0.023744                31.000000   \n",
      "50%         2318.044763     0.200000               155.500000   \n",
      "75%         2986.712085     0.620719               581.000000   \n",
      "max        17935.148148     1.000000              7112.000000   \n",
      "\n",
      "       total_participant  total_correct_sub_for_each_q  total_sub_for_each_q  \\\n",
      "count        4700.000000                   4700.000000           4700.000000   \n",
      "mean         4878.868936                   1529.629574           1976.303404   \n",
      "std          4295.019058                   2487.934386           2875.007369   \n",
      "min            18.000000                      1.000000              1.000000   \n",
      "25%          1015.000000                     49.000000            110.000000   \n",
      "50%          4022.000000                    377.000000            585.500000   \n",
      "75%          6544.750000                   2076.500000           2973.250000   \n",
      "max         19332.000000                  16217.000000          16747.000000   \n",
      "\n",
      "       total_time_to_sove_q  \n",
      "count          4.700000e+03  \n",
      "mean           2.505433e+06  \n",
      "std            3.628160e+06  \n",
      "min            9.000000e+00  \n",
      "25%            1.293932e+05  \n",
      "50%            8.312790e+05  \n",
      "75%            3.606592e+06  \n",
      "max            2.817065e+07  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# custum code for extracting data of sigle constes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "data = pd.DataFrame(columns=['contest_id', 'q_rating', 'contest_type', 'question_no', 'accuracy',\r\n",
    "                             'avg_solving_time', 'sub_ratio', 'tried_but_cannot_solved', 'total_participant',\r\n",
    "                             'total_correct_sub_for_each_q', 'total_sub_for_each_q', 'total_time_to_sove_q'])\r\n",
    "#sub_ratio=correct sub/total participnat\r\n",
    "#accuracy=correct sub/total sub\r\n",
    "#avg sol time=sum of time of accepted sol/no of accepted sol\r\n",
    "id = \"1592\"\r\n",
    "# print(cnt,id)\r\n",
    "# id=\"1526\"\r\n",
    "standing = requests.get(\r\n",
    "    \"https://codeforces.com/api/contest.standings?contestId=\"+id)\r\n",
    "\r\n",
    "# print(standing.json()\r\n",
    "json_file = standing.json()\r\n",
    "re = json_file['result']['rows']\r\n",
    "contest_name = json_file['result']['contest']['name']\r\n",
    "# useful data\r\n",
    "total_participant = len(re)\r\n",
    "total_correct_sub_for_each_question = {}\r\n",
    "problem_rating = {}\r\n",
    "tried_but_not_solved = {}\r\n",
    "total_time_to_solve_q = {}\r\n",
    "total_sub_for_each_question = {}\r\n",
    "avg_solve_time_for_each_question = {}\r\n",
    "total_no_of_question = 0\r\n",
    "accuracy = {}\r\n",
    "\r\n",
    "q_list = json_file['result']['problems']\r\n",
    "for i in q_list:\r\n",
    "  # print(i['index'],i['rating'])\r\n",
    "  if 'rating' in i:\r\n",
    "    problem_rating[total_no_of_question+1] = i['rating']\r\n",
    "  else:\r\n",
    "    problem_rating[total_no_of_question+1] = -1\r\n",
    "  total_no_of_question += 1\r\n",
    "  total_time_to_solve_q[total_no_of_question] = 0\r\n",
    "  total_correct_sub_for_each_question[total_no_of_question] = 0\r\n",
    "  tried_but_not_solved[total_no_of_question] = 0\r\n",
    "  total_sub_for_each_question[total_no_of_question] = 0\r\n",
    "  accuracy[total_no_of_question] = 0\r\n",
    "# print(problem_rating)\r\n",
    "# print(total_no_of_question)\r\n",
    "# print(re)\r\n",
    "for i in re:\r\n",
    "  user_sub_for_each_q = i['problemResults']\r\n",
    "  question_id = 1\r\n",
    "  temp = []\r\n",
    "  for i in user_sub_for_each_q:\r\n",
    "    if('bestSubmissionTimeSeconds' in i):\r\n",
    "      temp.append([i['bestSubmissionTimeSeconds'], question_id])\r\n",
    "      total_correct_sub_for_each_question[question_id] += 1\r\n",
    "    else:\r\n",
    "      temp.append([0, question_id])\r\n",
    "      if i['rejectedAttemptCount'] > 0:\r\n",
    "        tried_but_not_solved[question_id] += 1\r\n",
    "    question_id += 1\r\n",
    "  temp.sort()\r\n",
    "  prev_time = 0\r\n",
    "  for [x, y] in temp:\r\n",
    "    total_time_to_solve_q[y] += x-prev_time\r\n",
    "    prev_time = x\r\n",
    "  # print(total_time_to_solve_q)\r\n",
    "for i in range(total_no_of_question):\r\n",
    "  total_sub_for_each_question[i +\r\n",
    "                              1] = total_correct_sub_for_each_question[i+1]+tried_but_not_solved[i+1]\r\n",
    "  if total_sub_for_each_question[i+1] > 0:\r\n",
    "    accuracy[i+1] = total_correct_sub_for_each_question[i+1] / \\\r\n",
    "        total_sub_for_each_question[i+1]\r\n",
    "\r\n",
    "#output\r\n",
    "# print(total_participant)\r\n",
    "# print(total_correct_sub_for_each_question)\r\n",
    "# print(total_sub_for_each_question)\r\n",
    "# print(tried_but_not_solved)\r\n",
    "# print(total_time_to_solve_q)\r\n",
    "# print(\"avg solving time in second\")\r\n",
    "for i in range(total_no_of_question):\r\n",
    "  if(total_correct_sub_for_each_question[i+1] > 0):\r\n",
    "    avg_solve_time_for_each_question[i+1] = total_time_to_solve_q[i+1] / \\\r\n",
    "        total_correct_sub_for_each_question[i+1]\r\n",
    "  else:\r\n",
    "    avg_solve_time_for_each_question[i+1] = 0\r\n",
    "# print(avg_solve_time_for_each_question)\r\n",
    "contest_type = 4  # for all other contest\r\n",
    "if \"Div. 3\" in contest_name:\r\n",
    "  contest_type = 3\r\n",
    "if \"Div. 2\" in contest_name:\r\n",
    "  contest_type = 2\r\n",
    "if \"Div. 1\" in contest_name:\r\n",
    "  contest_type = 1\r\n",
    "if \"Global Round\" in contest_name:\r\n",
    "  contest_type = 0\r\n",
    "for i in range(total_no_of_question):\r\n",
    "  data = data.append({'contest_id': id, 'q_rating': problem_rating[i+1], 'contest_type': contest_type, 'question_no': i+1, 'accuracy': accuracy[i+1],\r\n",
    "                      'avg_solving_time': avg_solve_time_for_each_question[i+1], 'sub_ratio': total_correct_sub_for_each_question[i+1]/total_participant,\r\n",
    "                      'tried_but_cannot_solved': tried_but_not_solved[i+1], 'total_participant': total_participant, 'total_correct_sub_for_each_q':\r\n",
    "                      total_correct_sub_for_each_question[i+1], 'total_sub_for_each_q': total_sub_for_each_question[i+1], 'total_time_to_sove_q':\r\n",
    "                      total_time_to_solve_q[i+1]}, ignore_index=True)\r\n",
    "# print(data)\r\n",
    "data = data.drop(['q_rating'], axis=1)\r\n",
    "print(data)\r\n",
    "# print(taken)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  contest_id contest_type question_no  accuracy  avg_solving_time  sub_ratio  \\\n",
      "0       1592            2           1  0.801662       1471.819655   0.785313   \n",
      "1       1592            2           2  0.669834       2034.840284   0.435173   \n",
      "2       1592            2           3  0.559815       2902.853960   0.113377   \n",
      "3       1592            2           4  0.339250       3085.116279   0.016090   \n",
      "4       1592            2           5  0.280702       3152.575000   0.007484   \n",
      "5       1592            2           6  0.500000       1307.933333   0.001403   \n",
      "6       1592            2           7  0.666667       1621.500000   0.000561   \n",
      "\n",
      "  tried_but_cannot_solved total_participant total_correct_sub_for_each_q  \\\n",
      "0                    2077             10690                         8395   \n",
      "1                    2293             10690                         4652   \n",
      "2                     953             10690                         1212   \n",
      "3                     335             10690                          172   \n",
      "4                     205             10690                           80   \n",
      "5                      15             10690                           15   \n",
      "6                       3             10690                            6   \n",
      "\n",
      "  total_sub_for_each_q total_time_to_sove_q  \n",
      "0                10472             12355926  \n",
      "1                 6945              9466077  \n",
      "2                 2165              3518259  \n",
      "3                  507               530640  \n",
      "4                  285               252206  \n",
      "5                   30                19619  \n",
      "6                    9                 9729  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# traning model (random forest alog)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = RandomForestRegressor()\r\n",
    "train_x_forest, test_x_forest, train_y_forest, test_y_forest = train_test_split(\r\n",
    "    x, y, test_size=0.2, random_state=42)\r\n",
    "model.fit(train_x_forest, train_y_forest)\r\n",
    "y_predict_forest = model.predict(test_x_forest)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# output for div 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x_div1=df_div1.drop(['q_rating'],axis=1)\r\n",
    "# x_div1=x_div1.drop(['contest_id'],axis=1)\r\n",
    "y_div1=df_div1['q_rating']\r\n",
    "# print(x_div1.head())\r\n",
    "# print(y_div1.head())\r\n",
    "model=RandomForestRegressor()\r\n",
    "train_x_forest_div1,test_x_forest_div1,train_y_forest_div1,test_y_forest_div1=train_test_split(x_div1,y_div1,test_size=0.2,random_state=42)\r\n",
    "model.fit(train_x_forest_div1,train_y_forest_div1)\r\n",
    "print(test_x_forest_div1.head())\r\n",
    "y_predict_forest_div1=model.predict(test_x_forest_div1)\r\n",
    "# test_x_forest_div11=[[1,5,0.056\t,4440,0.00709219858,22,1,23]]\r\n",
    "# y_predict_forest_div11=model.predict(test_x_forest_div11)\r\n",
    "# print(y_predict_forest_div11)\r\n",
    "# y_predict\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      contest_id  contest_type  question_no  accuracy  avg_solving_time  \\\n",
      "4771         715             1            1  0.982249       1704.168675   \n",
      "3058        1037             1            6  0.565543       2592.940397   \n",
      "3147        1010             1            1  0.753022        942.013761   \n",
      "3624         933             1            4  0.761905       4361.812500   \n",
      "3535         949             1            2  0.969828       2295.920000   \n",
      "\n",
      "      sub_ratio  tried_but_cannot_solved  total_participant  \\\n",
      "4771   0.930841                        9                535   \n",
      "3058   0.030164                      116               5006   \n",
      "3147   0.742760                      143                587   \n",
      "3624   0.029575                        5                541   \n",
      "3535   0.875486                       14                514   \n",
      "\n",
      "      total_correct_sub_for_each_q  total_sub_for_each_q  total_time_to_sove_q  \n",
      "4771                           498                   507                848676  \n",
      "3058                           151                   267                391534  \n",
      "3147                           436                   579                410718  \n",
      "3624                            16                    21                 69789  \n",
      "3535                           450                   464               1033164  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# output for div 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "x_div2 = df_div2.drop(['q_rating'], axis=1)\r\n",
    "# x_div2=x_div2.drop(['contest_id'],axis=1)\r\n",
    "y_div2 = df_div2['q_rating']\r\n",
    "model = RandomForestRegressor()\r\n",
    "train_x_forest_div2, test_x_forest_div2, train_y_forest_div2, test_y_forest_div2 = train_test_split(\r\n",
    "    x_div2, y_div2, test_size=0.05, random_state=42)\r\n",
    "# print(train_x_forest_div2.describe())\r\n",
    "model.fit(train_x_forest_div2, train_y_forest_div2)\r\n",
    "y_predict_forest_div2 = model.predict(test_x_forest_div2)\r\n",
    "# y_predict\r\n",
    "y_predict_forest_div2 = model.predict(data)\r\n",
    "y_predict_forest_div2\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 815., 1256., 1800., 2093., 2270., 2582., 2773.])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# output div 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "x_div3 = df_div3.drop(['q_rating'], axis=1)\r\n",
    "# x_div2=x_div2.drop(['contest_id'],axis=1)\r\n",
    "y_div3 = df_div3['q_rating']\r\n",
    "model = RandomForestRegressor()\r\n",
    "train_x_forest_div3, test_x_forest_div3, train_y_forest_div3, test_y_forest_div3 = train_test_split(\r\n",
    "    x_div3, y_div3, test_size=0.2, random_state=42)\r\n",
    "model.fit(train_x_forest_div3, train_y_forest_div3)\r\n",
    "y_predict_forest_div3 = model.predict(test_x_forest_div3)\r\n",
    "# y_predict\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3fb7de5e7d60a9982dc76edf1c321b7b73ad9b771ff0939a5a6d2357aea103d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}