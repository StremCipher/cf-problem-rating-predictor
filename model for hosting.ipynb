{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import rename, times\n",
    "# from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        contest_id     q_rating  contest_type  question_no     accuracy  \\\n",
      "count  4700.000000  4700.000000   4700.000000  4700.000000  4700.000000   \n",
      "mean   1102.369149  1891.595745      2.234894     3.822340     0.634800   \n",
      "std     251.454666   677.331562      1.077807     2.253873     0.258493   \n",
      "min     641.000000   800.000000      0.000000     1.000000     0.002401   \n",
      "25%     886.000000  1400.000000      2.000000     2.000000     0.457588   \n",
      "50%    1103.500000  1900.000000      2.000000     4.000000     0.678199   \n",
      "75%    1322.000000  2400.000000      3.000000     5.000000     0.853926   \n",
      "max    1529.000000  3500.000000      4.000000    16.000000     1.000000   \n",
      "\n",
      "       avg_solving_time    sub_ratio  tried_but_cannot_solved  \\\n",
      "count       4700.000000  4700.000000              4700.000000   \n",
      "mean        2451.258099     0.327319               446.673830   \n",
      "std         1302.181881     0.330178               694.320172   \n",
      "min            9.000000     0.000070                 0.000000   \n",
      "25%         1661.950253     0.023744                31.000000   \n",
      "50%         2318.044763     0.200000               155.500000   \n",
      "75%         2986.712085     0.620719               581.000000   \n",
      "max        17935.148148     1.000000              7112.000000   \n",
      "\n",
      "       total_participant  total_correct_sub_for_each_q  total_sub_for_each_q  \\\n",
      "count        4700.000000                   4700.000000           4700.000000   \n",
      "mean         4878.868936                   1529.629574           1976.303404   \n",
      "std          4295.019058                   2487.934386           2875.007369   \n",
      "min            18.000000                      1.000000              1.000000   \n",
      "25%          1015.000000                     49.000000            110.000000   \n",
      "50%          4022.000000                    377.000000            585.500000   \n",
      "75%          6544.750000                   2076.500000           2973.250000   \n",
      "max         19332.000000                  16217.000000          16747.000000   \n",
      "\n",
      "       total_time_to_sove_q  \n",
      "count          4.700000e+03  \n",
      "mean           2.505433e+06  \n",
      "std            3.628160e+06  \n",
      "min            9.000000e+00  \n",
      "25%            1.293932e+05  \n",
      "50%            8.312790e+05  \n",
      "75%            3.606592e+06  \n",
      "max            2.817065e+07  \n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "df = pd.read_csv('merged-1-to-16.csv')\n",
    "#remove data of unrated round like technocup etc.....\n",
    "# df=df.reset_index()\n",
    "# print(df.head())\n",
    "# droping all row having any negative value in column\n",
    "df = df[df.select_dtypes(include=[np.number]).ge(0).all(1)]\n",
    "# droping row having avg solving time is gratar than 10 hr and less than equal to 0\n",
    "df = df.loc[(df['avg_solving_time'] <= 18000) & (df['avg_solving_time'] > 0)]\n",
    "df_div1 = df.loc[(df['contest_type'] == 1)]\n",
    "df_div2 = df.loc[(df['contest_type'] == 2)]\n",
    "df_div3 = df.loc[(df['contest_type'] == 3)]\n",
    "# df_div3.describe()\n",
    "# df.head()\n",
    "# df.describe()\n",
    "# print(df.head())\n",
    "print(df.describe())\n",
    "q_1 = df.loc[:, ['question_no', 'q_rating', 'contest_type']]\n",
    "q_1 = q_1.loc[(q_1['question_no'] == 1) & (q_1['contest_type'] == 1)]\n",
    "# print(q_1)\n",
    "# print(q_1.describe())\n",
    "x = df.drop(['q_rating'], axis=1)\n",
    "x = x.drop(['contest_id'], axis=1)\n",
    "y = df['q_rating']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custum code for extracting data of sigle constes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  contest_id contest_type question_no  accuracy  avg_solving_time  sub_ratio  \\\n",
      "0       1593            3           1  0.863397       1421.377581   0.830331   \n",
      "1       1593            3           2  0.881363       1718.848172   0.569522   \n",
      "2       1593            3           3  0.895678       1596.573930   0.514412   \n",
      "3       1593            3           4  0.810504       1505.190655   0.409795   \n",
      "4       1593            3           5  0.456481       2358.979716   0.032893   \n",
      "5       1593            3           6  0.563972       2326.867946   0.118228   \n",
      "6       1593            3           7  0.676471       2185.449275   0.004604   \n",
      "7       1593            3           8  0.780000       1918.205128   0.002602   \n",
      "\n",
      "  tried_but_cannot_solved total_participant total_correct_sub_for_each_q  \\\n",
      "0                    1969             14988                        12445   \n",
      "1                    1149             14988                         8536   \n",
      "2                     898             14988                         7710   \n",
      "3                    1436             14988                         6142   \n",
      "4                     587             14988                          493   \n",
      "5                    1370             14988                         1772   \n",
      "6                      33             14988                           69   \n",
      "7                      11             14988                           39   \n",
      "\n",
      "  total_sub_for_each_q total_time_to_sove_q  \n",
      "0                14414             17689044  \n",
      "1                 9685             14672088  \n",
      "2                 8608             12309585  \n",
      "3                 7578              9244881  \n",
      "4                 1080              1162977  \n",
      "5                 3142              4123210  \n",
      "6                  102               150796  \n",
      "7                   50                74810  \n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=['contest_id', 'q_rating', 'contest_type', 'question_no', 'accuracy',\n",
    "                             'avg_solving_time', 'sub_ratio', 'tried_but_cannot_solved', 'total_participant',\n",
    "                             'total_correct_sub_for_each_q', 'total_sub_for_each_q', 'total_time_to_sove_q'])\n",
    "#sub_ratio=correct sub/total participnat\n",
    "#accuracy=correct sub/total sub\n",
    "#avg sol time=sum of time of accepted sol/no of accepted \n",
    "# print(cnt,id)\n",
    "# id=\"1526\"\n",
    "standing = requests.get(\n",
    "    \"https://codeforces.com/api/contest.standings?contestId=\"+id)\n",
    "\n",
    "# print(standing.json()\n",
    "json_file = standing.json()\n",
    "re = json_file['result']['rows']\n",
    "contest_name = json_file['result']['contest']['name']\n",
    "# useful data\n",
    "total_participant = len(re)\n",
    "total_correct_sub_for_each_question = {}\n",
    "problem_rating = {}\n",
    "tried_but_not_solved = {}\n",
    "total_time_to_solve_q = {}\n",
    "total_sub_for_each_question = {}\n",
    "avg_solve_time_for_each_question = {}\n",
    "total_no_of_question = 0\n",
    "accuracy = {}\n",
    "\n",
    "q_list = json_file['result']['problems']\n",
    "for i in q_list:\n",
    "  # print(i['index'],i['rating'])\n",
    "  if 'rating' in i:\n",
    "    problem_rating[total_no_of_question+1] = i['rating']\n",
    "  else:\n",
    "    problem_rating[total_no_of_question+1] = -1\n",
    "  total_no_of_question += 1\n",
    "  total_time_to_solve_q[total_no_of_question] = 0\n",
    "  total_correct_sub_for_each_question[total_no_of_question] = 0\n",
    "  tried_but_not_solved[total_no_of_question] = 0\n",
    "  total_sub_for_each_question[total_no_of_question] = 0\n",
    "  accuracy[total_no_of_question] = 0\n",
    "# print(problem_rating)\n",
    "# print(total_no_of_question)\n",
    "# print(re)\n",
    "for i in re:\n",
    "  user_sub_for_each_q = i['problemResults']\n",
    "  question_id = 1\n",
    "  temp = []\n",
    "  for i in user_sub_for_each_q:\n",
    "    if('bestSubmissionTimeSeconds' in i):\n",
    "      temp.append([i['bestSubmissionTimeSeconds'], question_id])\n",
    "      total_correct_sub_for_each_question[question_id] += 1\n",
    "    else:\n",
    "      temp.append([0, question_id])\n",
    "      if i['rejectedAttemptCount'] > 0:\n",
    "        tried_but_not_solved[question_id] += 1\n",
    "    question_id += 1\n",
    "  temp.sort()\n",
    "  prev_time = 0\n",
    "  for [x, y] in temp:\n",
    "    total_time_to_solve_q[y] += x-prev_time\n",
    "    prev_time = x\n",
    "  # print(total_time_to_solve_q)\n",
    "for i in range(total_no_of_question):\n",
    "  total_sub_for_each_question[i +\n",
    "                              1] = total_correct_sub_for_each_question[i+1]+tried_but_not_solved[i+1]\n",
    "  if total_sub_for_each_question[i+1] > 0:\n",
    "    accuracy[i+1] = total_correct_sub_for_each_question[i+1] / \\\n",
    "        total_sub_for_each_question[i+1]\n",
    "\n",
    "#output\n",
    "# print(total_participant)\n",
    "# print(total_correct_sub_for_each_question)\n",
    "# print(total_sub_for_each_question)\n",
    "# print(tried_but_not_solved)\n",
    "# print(total_time_to_solve_q)\n",
    "# print(\"avg solving time in second\")\n",
    "for i in range(total_no_of_question):\n",
    "  if(total_correct_sub_for_each_question[i+1] > 0):\n",
    "    avg_solve_time_for_each_question[i+1] = total_time_to_solve_q[i+1] / \\\n",
    "        total_correct_sub_for_each_question[i+1]\n",
    "  else:\n",
    "    avg_solve_time_for_each_question[i+1] = 0\n",
    "# print(avg_solve_time_for_each_question)\n",
    "contest_type = 4  # for all other contest\n",
    "if \"Div. 3\" in contest_name:\n",
    "  contest_type = 3\n",
    "if \"Div. 2\" in contest_name:\n",
    "  contest_type = 2\n",
    "if \"Div. 1\" in contest_name:\n",
    "  contest_type = 1\n",
    "if \"Global Round\" in contest_name:\n",
    "  contest_type = 0\n",
    "for i in range(total_no_of_question):\n",
    "  data = data.append({'contest_id': id, 'q_rating': problem_rating[i+1], 'contest_type': contest_type, 'question_no': i+1, 'accuracy': accuracy[i+1],\n",
    "                      'avg_solving_time': avg_solve_time_for_each_question[i+1], 'sub_ratio': total_correct_sub_for_each_question[i+1]/total_participant,\n",
    "                      'tried_but_cannot_solved': tried_but_not_solved[i+1], 'total_participant': total_participant, 'total_correct_sub_for_each_q':\n",
    "                      total_correct_sub_for_each_question[i+1], 'total_sub_for_each_q': total_sub_for_each_question[i+1], 'total_time_to_sove_q':\n",
    "                      total_time_to_solve_q[i+1]}, ignore_index=True)\n",
    "# print(data)\n",
    "data = data.drop(['q_rating'], axis=1)\n",
    "print(data)\n",
    "# print(taken)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traning model (random forest alog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dfd125cedda5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_x_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_forest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_predict_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x_forest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \"\"\"\n\u001b[0;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[0;32m    196\u001b[0m                             \" a valid collection.\" % x)\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(0) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "train_x_forest, test_x_forest, train_y_forest, test_y_forest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "model.fit(train_x_forest, train_y_forest)\n",
    "y_predict_forest = model.predict(test_x_forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output for div 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      contest_id  contest_type  question_no  accuracy  avg_solving_time  \\\n",
      "4771         715             1            1  0.982249       1704.168675   \n",
      "3058        1037             1            6  0.565543       2592.940397   \n",
      "3147        1010             1            1  0.753022        942.013761   \n",
      "3624         933             1            4  0.761905       4361.812500   \n",
      "3535         949             1            2  0.969828       2295.920000   \n",
      "\n",
      "      sub_ratio  tried_but_cannot_solved  total_participant  \\\n",
      "4771   0.930841                        9                535   \n",
      "3058   0.030164                      116               5006   \n",
      "3147   0.742760                      143                587   \n",
      "3624   0.029575                        5                541   \n",
      "3535   0.875486                       14                514   \n",
      "\n",
      "      total_correct_sub_for_each_q  total_sub_for_each_q  total_time_to_sove_q  \n",
      "4771                           498                   507                848676  \n",
      "3058                           151                   267                391534  \n",
      "3147                           436                   579                410718  \n",
      "3624                            16                    21                 69789  \n",
      "3535                           450                   464               1033164  \n"
     ]
    }
   ],
   "source": [
    "x_div1=df_div1.drop(['q_rating'],axis=1)\n",
    "# x_div1=x_div1.drop(['contest_id'],axis=1)\n",
    "y_div1=df_div1['q_rating']\n",
    "# print(x_div1.head())\n",
    "# print(y_div1.head())\n",
    "model=RandomForestRegressor()\n",
    "train_x_forest_div1,test_x_forest_div1,train_y_forest_div1,test_y_forest_div1=train_test_split(x_div1,y_div1,test_size=0.2,random_state=42)\n",
    "model.fit(train_x_forest_div1,train_y_forest_div1)\n",
    "print(test_x_forest_div1.head())\n",
    "y_predict_forest_div1=model.predict(test_x_forest_div1)\n",
    "# test_x_forest_div11=[[1,5,0.056\t,4440,0.00709219858,22,1,23]]\n",
    "# y_predict_forest_div11=model.predict(test_x_forest_div11)\n",
    "# print(y_predict_forest_div11)\n",
    "# y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output for div 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 801., 1116., 1154., 1341., 2121., 1793., 2468., 2580.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_div2 = df_div2.drop(['q_rating'], axis=1)\n",
    "# x_div2=x_div2.drop(['contest_id'],axis=1)\n",
    "y_div2 = df_div2['q_rating']\n",
    "model = RandomForestRegressor()\n",
    "train_x_forest_div2, test_x_forest_div2, train_y_forest_div2, test_y_forest_div2 = train_test_split(\n",
    "    x_div2, y_div2, test_size=0.05, random_state=42)\n",
    "# print(train_x_forest_div2.describe())\n",
    "model.fit(train_x_forest_div2, train_y_forest_div2)\n",
    "# y_predict_forest_div2 = model.predict(test_x_forest_div2)\n",
    "# y_predict\n",
    "y_predict_forest_div2 = model.predict(data)\n",
    "y_predict_forest_div2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output div 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 815., 1048., 1053., 1169., 1821., 1532., 2151., 2158.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_div3 = df_div3.drop(['q_rating'], axis=1)\n",
    "# x_div2=x_div2.drop(['contest_id'],axis=1)\n",
    "y_div3 = df_div3['q_rating']\n",
    "# print(y_div3)\n",
    "model = RandomForestRegressor()\n",
    "train_x_forest_div3, test_x_forest_div3, train_y_forest_div3, test_y_forest_div3 = train_test_split(\n",
    "    x_div3, y_div3, test_size=0.2, random_state=42)\n",
    "model.fit(train_x_forest_div3, train_y_forest_div3)\n",
    "y_predict_forest_div3 = model.predict(data)\n",
    "# y_predict\n",
    "y_predict_forest_div3\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fb7de5e7d60a9982dc76edf1c321b7b73ad9b771ff0939a5a6d2357aea103d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
